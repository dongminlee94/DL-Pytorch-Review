{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd : Automatic differentiation\n",
    "\n",
    "Autograd를 사용하면 backprop을 위한 미분 값을 자동으로 게산해줍니다.\n",
    "자동 계산을 위해서 사용하는 변수는 **torch.autograd에 있는 Variable** 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 기본세팅을 하겠습니다. import는 다음과 같이 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.ones(2,2)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable의 표현은 기본표현과 requires_grad를 포함한 표현으로 총 두 가지가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = Variable(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requires_grad를 포함한 표현(왠만하면 requires_grad를 쓰는 것을 추천해드립니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = Variable(a1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 requires_grad는 **'a값에 대한 gradient값이 필요하다.'** 라는 의미입니다.\n",
    "나중에 CNN, RNN을 구성할 때 필요한 필터들 같은 경우 이미 w값을 가지고 있습니다.\n",
    "자동으로 requires_grad를 품고 있습니다. 따라서 따로 선언 해줄 필요는 없습니다.\n",
    "**하지만 기본적인 변수인 a1는 따로 requires_grad를 해주지 않으면\n",
    "gradient값을 생성하지 않습니다.**\n",
    "또한 **Variable(a1)만 하게 된다면 나중에 backward 진행할 시 에러가 납니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd.Variable의 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd.Variable은 총 세 가지를 포함합니다.\n",
    "\n",
    "* data\n",
    "    * 위에서 torch.ones(2,2)의 형태로써 Tensor형태의 데이터가 담깁니다.\n",
    "* grad\n",
    "    * Data가 거쳐온 layer에 대한 미분값이 축적됩니다. \n",
    "    * 업데이트를 위한 gradient descent value이 들어갑니다. \n",
    "    * autograd.Variable으로 표현을 한 변수**만** 이 부분에 gradient descent 값이 남습니다.\n",
    "* grad_fn\n",
    "    * 미분 값을 계산한 함수에 대한 정보를 담습니다. '어떤 연산에 대한 backward를 진행을 했다' 라는 표현으로 설명할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - a1.data - - - - - - - - - -\n",
      "\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "- - - - - - - - - - a1.grad - - - - - - - - - -\n",
      "None\n",
      "- - - - - - - - - - a1.grad_fn - - - - - - - - - -\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"- - - - - - - - - - a1.data - - - - - - - - - -\")\n",
    "print(a1.data)\n",
    "\n",
    "print(\"- - - - - - - - - - a1.grad - - - - - - - - - -\")\n",
    "print(a1.grad)\n",
    "\n",
    "print(\"- - - - - - - - - - a1.grad_fn - - - - - - - - - -\")\n",
    "print(a1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b1 = a1+2\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 9  9\n",
      " 9  9\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c1 = b1**2\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 36\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = c1.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward의 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward : a1 -> b1 -> c1 -> out이고, backward를 통해 out'/a1'('s는 미분)값이 a1.grad에 담기게 됩니다. 그러면 아까 a1.grad의 None값에 어떠한 값이 채워지게 됩니다. 만약 a1 = Variable(a1, requires_grad=True)에서 requires_grad=True를 하지 않으면 이부분에서 에러가 납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 36\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward 실행 후, Variable의 의미 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - a1.data - - - - - - - - - -\n",
      "\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "- - - - - - - - - - a1.grad - - - - - - - - - -\n",
      "Variable containing:\n",
      " 6  6\n",
      " 6  6\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "- - - - - - - - - - a1.grad_fn - - - - - - - - - -\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"- - - - - - - - - - a1.data - - - - - - - - - -\")\n",
    "print(a1.data)\n",
    "\n",
    "print(\"- - - - - - - - - - a1.grad - - - - - - - - - -\")\n",
    "'''\n",
    "이 부분이 채워집니다.\n",
    "'''\n",
    "print(a1.grad) \n",
    "\n",
    "print(\"- - - - - - - - - - a1.grad_fn - - - - - - - - - -\")\n",
    "print(a1.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - b1.data - - - - - - - - - -\n",
      "\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "- - - - - - - - - - b1.grad - - - - - - - - - -\n",
      "None\n",
      "- - - - - - - - - - b1.grad_fn - - - - - - - - - -\n",
      "<AddBackward0 object at 0x7f77257d1e80>\n"
     ]
    }
   ],
   "source": [
    "print(\"- - - - - - - - - - b1.data - - - - - - - - - -\")\n",
    "print(b1.data)\n",
    "\n",
    "print(\"- - - - - - - - - - b1.grad - - - - - - - - - -\")\n",
    "'''\n",
    "gradient부분이 필요로 하지 않았기 때문에 None인 것입니다. \n",
    "이전에 Variable로 한 것이 a1밖에 없었기 때문에 a1만 나오는 것입니다.\n",
    "'''\n",
    "print(b1.grad) \n",
    "\n",
    "print(\"- - - - - - - - - - b1.grad_fn - - - - - - - - - -\")\n",
    "'''\n",
    "이 부분이 채워집니다.\n",
    "b = a+2였기 때문에 'AddBackward 연산을 했다' 라는 표현으로 볼 수 있습니다.\n",
    "'''\n",
    "print(b1.grad_fn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - c1.data - - - - - - - - - -\n",
      "\n",
      " 9  9\n",
      " 9  9\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "- - - - - - - - - - c1.grad - - - - - - - - - -\n",
      "None\n",
      "- - - - - - - - - - c1.grad_fn - - - - - - - - - -\n",
      "<PowBackward0 object at 0x7f7700c90710>\n"
     ]
    }
   ],
   "source": [
    "print(\"- - - - - - - - - - c1.data - - - - - - - - - -\")\n",
    "print(c1.data)\n",
    "\n",
    "print(\"- - - - - - - - - - c1.grad - - - - - - - - - -\")\n",
    "print(c1.grad) \n",
    "\n",
    "print(\"- - - - - - - - - - c1.grad_fn - - - - - - - - - -\")\n",
    "print(c1.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - out.data - - - - - - - - - -\n",
      "\n",
      " 36\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "- - - - - - - - - - out.grad - - - - - - - - - -\n",
      "None\n",
      "- - - - - - - - - - out.grad_fn - - - - - - - - - -\n",
      "<SumBackward0 object at 0x7f7700c90860>\n"
     ]
    }
   ],
   "source": [
    "print(\"- - - - - - - - - - out.data - - - - - - - - - -\")\n",
    "print(out.data)\n",
    "\n",
    "print(\"- - - - - - - - - - out.grad - - - - - - - - - -\")\n",
    "print(out.grad) \n",
    "\n",
    "print(\"- - - - - - - - - - out.grad_fn - - - - - - - - - -\")\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한번에 정리해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3,3)\n",
    "x = Variable(x, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3  3\n",
      " 3  3  3\n",
      " 3  3  3\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "z = y*3\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0.1000\n",
      "  1.0000\n",
      " 10.0000\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grad = torch.Tensor([0.1,1,10])\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = 3 * x^2\n",
    "미분하면 z'/x' = 3 * 2 * x\n",
    "위에서 x가 'x = torch.ones(3,3)' 였으므로, 1을 대입하면 z'/x' = 6입니다.\n",
    "여기서 z.backward(grad)를 하면 (tf에서 feet_dict처럼 대입을 하여) 6에 대해서 각각 곱해집니다.\n",
    "'grad는 반드시 x와 shape이 동일해야 합니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1\n",
      " 1  1  1\n",
      " 1  1  1\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "Variable containing:\n",
      "  0.6000   6.0000  60.0000\n",
      "  0.6000   6.0000  60.0000\n",
      "  0.6000   6.0000  60.0000\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.data)\n",
    "print(x.grad)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3  3\n",
      " 3  3  3\n",
      " 3  3  3\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
