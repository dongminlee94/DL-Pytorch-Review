{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network 더 쉽게 만들기(nn.Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 기본적인 code를 적어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np \n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10_data',\n",
    "                                                            train=True,\n",
    "                                                            download=True,\n",
    "                                                            transform=transform)\n",
    " \n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10_data',\n",
    "                                                            train=False,\n",
    "                                                            download=True,\n",
    "                                                            transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class를 정의할 때 nn.Sequential을 이용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에 사용했던 방법은 아래와 같이 사용하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 30, 5)\n",
    "        self.fc1 = nn.Linear(30*5*5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv2(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 이렇게 일일히 써야하고, 일일히 쓰게되면 forward에서도 변수들을 일일히 다 써줘야합니다. layer가 길어지면 계속 쓰기가 귀찮기도 하고 하나하나 잡아주기가 벅찹니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 **self.layer1 = nn.Sequential()** 와 같이 nn.Sequential()을 이용하여 아래와 같이 layer를 쉽게 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, 5),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(32, 64, 3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(64, 128, 3),\n",
    "        nn.Conv2d(128, 128, 3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(128, 256, 3),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 code로 작성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(3, 32, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer2_1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 7, 1, 2),\n",
    "            nn.Conv2d(512, 64, 1),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer2_2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 5, 1, 1),\n",
    "            nn.Conv2d(512, 64, 1),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.layer2_3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.Conv2d(512, 64, 1),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3*64*4*4, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.data.shape)\n",
    "        x = self.layer1(x)\n",
    "        x1 = self.layer2_1(x)\n",
    "        x2 = self.layer2_2(x)\n",
    "        x3 = self.layer2_3(x)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GPU\n",
    "# net = Net().cuda()\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x = torch.cat((x1, x2, x3), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "torch.cat(seq, dim=0, out=None) → Tensor\n",
    "        \n",
    "x = torch.randn(2, 3)\n",
    ">>> x\n",
    "\n",
    "  0.5983 -0.0341  2.4918\n",
    "  1.5981 -0.5265 -0.8735\n",
    " [torch.FloatTensor of size (2,3)]\n",
    "\n",
    ">>> torch.cat((x, x, x), 0)\n",
    "\n",
    "  0.5983 -0.0341  2.4918\n",
    "  1.5981 -0.5265 -0.8735\n",
    "  0.5983 -0.0341  2.4918\n",
    "  1.5981 -0.5265 -0.8735\n",
    "  0.5983 -0.0341  2.4918\n",
    "  1.5981 -0.5265 -0.8735\n",
    " [torch.FloatTensor of size (6,3)]\n",
    "\n",
    ">>> torch.cat((x, x, x), 1)\n",
    "\n",
    "  0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918\n",
    "  1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735\n",
    " [torch.FloatTensor of size (2,9)]\n",
    "\n",
    "\n",
    " x1, x2, x3는 B x C x H x W로 되어있습니다. 여기서 1은 'C(channel)로 정렬하겠다.' 라는 뜻입니다.\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x = x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "여기서도 x는 B x C x H x W로 되어있습니다. x.shape[0]은 B(batch_size)를 뜻하고\n",
    "batch_size를 제외하고 '나머지는 fc해야하니까 1차원(1줄)으로 쭉 펼쳐주세요.' 라는 뜻입니다.\n",
    "\n",
    "        x = x.view(x.shape[0], -1)        \n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 결과값이 잘 나오는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,3,32,32)\n",
    "print(a.shape)\n",
    "\n",
    "a = Variable(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "out = net(a)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 정한대로 [1, 10]로 잘 나오는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0835  2.0330  2.9364  0.5986 -1.3144  2.5163 -1.4628 -3.6521  0.5933 -0.5305\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 저장하고 불러와서 다시 쓰기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 out을 출력한 이유는 'net = Net()'의 값을 저장했다가 다시 불러서 연산을 했을 때 똑같이 나오도록 하기 위해서 출력한 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network를 쓰고, 저장하고, 불러오는 것을 해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './save_model/cnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('./save_model/cnn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0835  2.0330  2.9364  0.5986 -1.3144  2.5163 -1.4628 -3.6521  0.5933 -0.5305\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = model(a)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
