{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn과 nn.functional의 차이점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn 지원기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parameter\n",
    "* Containers \n",
    "* Conv\n",
    "* Pooling \n",
    "* Padding \n",
    "* Non-linear Activation\n",
    "* Normalization\n",
    "* Recurrent \n",
    "* Linear\n",
    "* Dropout\n",
    "* Sparse\n",
    "* Distance\n",
    "* Loss \n",
    "* Vision\n",
    "* Data Parallel\n",
    "* Utilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.functional 지원기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conv\n",
    "* Pooling\n",
    "* Non-linear activation\n",
    "* Normalization\n",
    "* Linear function(=Fully connected layer) \n",
    "* Dropout\n",
    "* Distance\n",
    "* Loss\n",
    "* Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d로 살펴본 차이점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn - torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가적으로 in_channels가 만약 (1,1,3,3)이라면 첫 번째 1은 batch_size, 두 번째 1은 channel의 개수 세 번째 3은 hight, 네 번째 3은 width입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.functional - torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 weight를 직접 지정해줘야 합니다. 따라서 외부에서 직접 filter(kernel_size)를 만들어주어야합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 구체적으로 먼저 nn.functional을 이용한 방법부터 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.functional을 이용한 3x3 Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  1  1  1\n",
      "  1  1  1\n",
      "  1  1  1\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones(1,1,3,3)\n",
    "filter = torch.ones(1,1,3,3)\n",
    "\n",
    "input = Variable(input, requires_grad=True)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  9\n",
      "[torch.FloatTensor of size 1x1x1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter는 w로 생각하기 때문에 따로 requires_grad를 하지 않습니다.\n",
    "filter = Variable(filter) \n",
    "\n",
    "out = F.conv2d(input, filter)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConvNdBackward object at 0x7f7f6a7eaa58>\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  1  1  1\n",
      "  1  1  1\n",
      "  1  1  1\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "\n",
    "print(out.grad_fn)\n",
    "\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn을 이용한 3x3 Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  1  1  1\n",
      "  1  1  1\n",
      "  1  1  1\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.ones(1,1,3,3)\n",
    "input1 = Variable(input1, requires_grad=True)\n",
    "print(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.1806 -0.1525  0.2559\n",
      "  0.2310 -0.0641 -0.1582\n",
      "  0.3043  0.1705 -0.2148\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func = nn.Conv2d(1,1,3)\n",
    "print(func.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   5.5095\n",
      "[torch.FloatTensor of size 1x1x1x1]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out1 = func(input1)\n",
    "print(out1)\n",
    "\n",
    "print(input1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.1806 -0.1525  0.2559\n",
      "  0.2310 -0.0641 -0.1582\n",
      "  0.3043  0.1705 -0.2148\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out1.backward()\n",
    "\n",
    "print(input1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5x5를 3x3 filter로 Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0934 -0.2952 -0.2234\n",
      " -0.1953  0.1912  0.3060\n",
      "  0.1122 -0.0918  0.2457\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input2 = torch.ones(1,1,5,5)\n",
    "input2 = Variable(input2, requires_grad=True)\n",
    "\n",
    "func = nn.Conv2d(1,1,3, bias=None)\n",
    "# 아직 채널이 없으므로 (1,1,?)로 고정\n",
    "print(func.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.functional로 weight값을 설정해줄 수도 있지만 nn으로도 이렇게 Conv2d의 weight를 직접 설정해줄 수 있습니다. 나중에 직접 layer를 만들거나 다른 네트워크에서 free train된 weight값에다가 직접 weight를 지정하여 집어넣어야 하는 상황이 발생하면 밑의 코드처럼 하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  2  2  2\n",
      "  2  2  2\n",
      "  2  2  2\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func.weight = torch.nn.Parameter(torch.ones(1,1,3,3) + 1)\n",
    "print(func.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  18  18  18\n",
      "  18  18  18\n",
      "  18  18  18\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out2 = func(input2)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f92619d17b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/env/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 new_grads.append(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "out2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight를 지정했기 때문에 backward가 진행되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLu, Max_Pooling, Sigmoid, Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.8566  0.2786  1.3763\n",
      "  0.1923 -0.7132  2.1710\n",
      " -1.3916  0.4761  0.9420\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.8566  0.2786  1.3763\n",
      "  0.1923  0.0000  2.1710\n",
      "  0.0000  0.4761  0.9420\n",
      "[torch.FloatTensor of size 1x1x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "act_input = Variable(torch.randn(1,1,3,3))\n",
    "print(act_input)\n",
    "\n",
    "act = F.relu(act_input)\n",
    "print(act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max_Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.8566  2.1710\n",
      "  0.4761  2.1710\n",
      "[torch.FloatTensor of size 1x1x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = nn.MaxPool2d(2, stride=1)\n",
    "m_out = m(act)\n",
    "\n",
    "print(m_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.7019  0.8976\n",
      "  0.6168  0.8976\n",
      "[torch.FloatTensor of size 1x1x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "act2 = F.sigmoid(m_out)\n",
    "print(act2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.6945  0.9743\n",
      "  0.4431  0.9743\n",
      "[torch.FloatTensor of size 1x1x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "act3 = F.tanh(m_out)\n",
    "print(act3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class로 묶어서 마지막으로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(model, self).__init__()\n",
    "    self.Max_Pooling = nn.MaxPool2d(2, stride=1)\n",
    "    self.Avg_Pooling = nn.MaxPool2d(2, stride=1)\n",
    "  def forward(self, x):\n",
    "    x = F.relu(x)\n",
    "    x = self.Max_Pooling(x)\n",
    "    x = F.tanh(x)\n",
    "    x = self.Avg_Pooling(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.9743\n",
      "[torch.FloatTensor of size 1x1x1x1]\n",
      "\n",
      "model(\n",
      "  (Max_Pooling): MaxPool2d(kernel_size=(2, 2), stride=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "  (Avg_Pooling): MaxPool2d(kernel_size=(2, 2), stride=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "out = model()\n",
    "print(out(act_input))\n",
    "\n",
    "# model로 어떤 것들을 선언했는 지를 출력해보겠습니다.\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
