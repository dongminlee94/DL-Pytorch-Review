{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss는 어디서 생길까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 다음을 가정하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input(입력) = x\n",
    "* Output(출력) = y\n",
    "* Label(정답) = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 우리는 입력과 정답만 가지고 있습니다.  \n",
    "\n",
    "2. 모델은 입력을 받아서 출력을 만듭니다. -> y = wx + b \n",
    "\n",
    "3. 우리는 d - y가 0이길 희망합니다. \n",
    "\n",
    "4. d - y != 0이면 문제가 있다고 봅니다 -> Error --> **Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error(=Loss)를 어떻게 계산하지? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 회귀(Regression) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 출력층에 쓰이는 활성화 함수\n",
    "* 항등사항(=나오는대로 씁니다.) \n",
    "\n",
    "tensorflow로는\n",
    "```python\n",
    "H = tf.matmul(X,W) + b\n",
    "```\n",
    "라고 보통 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Loss Function\n",
    "* MSE(Mean Squared Error)\n",
    "\n",
    "tensorflow로는 \n",
    "```python\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "```\n",
    "라고 보통 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 이중분류(Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 출력층에 쓰이는 활성화 함수\n",
    "- Logistic Function\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- Relu\n",
    "- Leaky Relu\n",
    "\n",
    "tensorflow로는\n",
    "```python\n",
    "H = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "```\n",
    "라고 보통 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Loss Function\n",
    "- Binary Cross Entropy Loss\n",
    "\n",
    "tensorflow로는\n",
    "```python\n",
    "cost = -tf.reduce_mean(Y*tf.log(H) + (1 - Y)*tf.log(1 - H))\n",
    "```\n",
    "라고 보통 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 다중분류(Multinomial Classification) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 출력층에 쓰이는 활성화 함수\n",
    "- Softmax\n",
    "\n",
    "tensorflow로는\n",
    "```python\n",
    "H = tf.nn.softmax(logits)\n",
    "```\n",
    "라고 보통 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Loss Function\n",
    "- (Multinomial) Cross Entropy \n",
    "\n",
    "tensorflow로는\n",
    "```python\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(H), axis=1))\n",
    "```\n",
    "또는 \n",
    "```python\n",
    "# 아래 코드로 할 때는 'H = tf.nn.softmax(logits)' 안해도 됩니다.\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=H, labels=Y))\n",
    "```\n",
    "라고 보통 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions (torch.nn.xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*은 많이 쓰이는 것을 표현하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1Loss *\n",
    "- MSELoss *\n",
    "- CrossEntropyLoss *\n",
    "- NLL(negative log likelihood)Loss\n",
    "- PoissonNLLLoss\n",
    "- NLLLoss2d\n",
    "- KLDivLoss\n",
    "- BCELoss *\n",
    "- BCEWithLogitsLoss\n",
    "- MarginRankingLoss\n",
    "- HingeEmbeddingLoss\n",
    "- MultiLabelMarginLoss\n",
    "- SmoothL1Loss\n",
    "- SoftMarginLoss\n",
    "- MultiLabelSoftMarginLoss\n",
    "- CosineEmbeddingLoss\n",
    "- MultiMarginLoss\n",
    "- TripletMarginLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer (torch.optim.xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*은 많이 쓰이는 것을 표현하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adadelta\n",
    "- Adagrad *\n",
    "- Adam *\n",
    "- SparseAdam\n",
    "- Adamax\n",
    "- ASGD\n",
    "- LBFGS \n",
    "- RMSprop *\n",
    "- Rprop\n",
    "- SGD *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optim을 사용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 코드진행의 순서를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import 하기 (import torch, torch.nn, torchvision, torch 등등)\n",
    "2. Dataset 만들기 (torchvision, torch.utils.data.Dataset & Loader)\n",
    "3. Model 만들기 (class MyNetwork(nn.Module): ~~)\n",
    "4. Optim과 Loss 계산 함수 결정하기\n",
    "5. 학습을 위한 반복문 작성하기\n",
    "6. 평가 및 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저는 여기서 CIFAR10으로 할 것이기 때문에 \n",
    "- Loss Function -> CrossEntropyLoss\n",
    "- Optimizer -> SGD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "으로 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(Net.parameters(), lr=0.001, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for epoch in range(num_epoch):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # 학습이 되기전에 0으로 초기화 해주는 부분입니다.\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        input, labels = data, testloader\n",
    "        input, labels = Variable(input), Variable(labels)\n",
    "\n",
    "        out = Net(input)\n",
    "\n",
    "        loss = loss(out, labels)\n",
    "        \n",
    "        # 자동으로 각 parameters(w, b)들이 Gradient값을 가집니다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 그 값을 가지고 parameters(w, b)들을 업데이트를 진행합니다. \n",
    "        optim.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 직접 짜보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np \n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10_data',\n",
    "                                                            train=True,\n",
    "                                                            download=True,\n",
    "                                                            transform=transform)\n",
    " \n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10_data',\n",
    "                                                            train=False,\n",
    "                                                            download=True,\n",
    "                                                            transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 30, 5)\n",
    "        self.fc1 = nn.Linear(30*5*5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv2(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        \n",
    "        # view에 대해서 설명을 하면 batch_size x 30 x 5 x 5이기 때문에 batch_size.(30 x 5 x 5)이런 식으로 됩니다.\n",
    "        # 앞에 batch_size를 통으로 fc1에 두기 위해서 하는 것 같습니다.\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        return x\n",
    "\n",
    "# GPU\n",
    "# net = Net().cuda()\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Optim과 Loss 계산 함수 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "# torch.optim.SGD(params) 여기에 model.parameter()를 반드시 해줘야합니다.\n",
    "optim = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 학습을 위한 반복문 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0=> loss : 2.308\n",
      "64=> loss : 2.317\n",
      "128=> loss : 2.310\n",
      "192=> loss : 2.243\n",
      "256=> loss : 2.226\n",
      "320=> loss : 2.267\n",
      "384=> loss : 2.200\n",
      "448=> loss : 2.190\n",
      "512=> loss : 2.227\n",
      "576=> loss : 2.241\n",
      "640=> loss : 1.817\n",
      "704=> loss : 1.692\n",
      "768=> loss : 1.900\n",
      "832=> loss : 2.377\n",
      "896=> loss : 2.108\n",
      "960=> loss : 1.967\n",
      "1024=> loss : 1.961\n",
      "1088=> loss : 1.776\n",
      "1152=> loss : 2.003\n",
      "1216=> loss : 1.721\n",
      "1280=> loss : 2.548\n",
      "1344=> loss : 1.941\n",
      "1408=> loss : 1.937\n",
      "1472=> loss : 2.297\n",
      "1536=> loss : 1.767\n",
      "1600=> loss : 1.979\n",
      "1664=> loss : 2.186\n",
      "1728=> loss : 2.052\n",
      "1792=> loss : 1.528\n",
      "1856=> loss : 1.609\n",
      "1920=> loss : 1.752\n",
      "1984=> loss : 2.582\n",
      "2048=> loss : 2.346\n",
      "2112=> loss : 1.965\n",
      "2176=> loss : 1.446\n",
      "2240=> loss : 1.588\n",
      "2304=> loss : 1.295\n",
      "2368=> loss : 1.149\n",
      "2432=> loss : 2.486\n",
      "2496=> loss : 1.959\n",
      "2560=> loss : 1.195\n",
      "2624=> loss : 1.980\n",
      "2688=> loss : 1.935\n",
      "2752=> loss : 1.578\n",
      "2816=> loss : 1.868\n",
      "2880=> loss : 1.405\n",
      "2944=> loss : 1.367\n",
      "3008=> loss : 1.876\n",
      "3072=> loss : 1.597\n",
      "3136=> loss : 1.584\n",
      "3200=> loss : 1.480\n",
      "3264=> loss : 1.201\n",
      "3328=> loss : 1.497\n",
      "3392=> loss : 1.823\n",
      "3456=> loss : 1.917\n",
      "3520=> loss : 1.815\n",
      "3584=> loss : 1.350\n",
      "3648=> loss : 1.359\n",
      "3712=> loss : 0.943\n",
      "3776=> loss : 1.352\n",
      "3840=> loss : 1.960\n",
      "3904=> loss : 1.042\n",
      "3968=> loss : 1.763\n",
      "4032=> loss : 1.613\n",
      "4096=> loss : 1.196\n",
      "4160=> loss : 1.221\n",
      "4224=> loss : 1.902\n",
      "4288=> loss : 1.872\n",
      "4352=> loss : 1.363\n",
      "4416=> loss : 1.984\n",
      "4480=> loss : 1.655\n",
      "4544=> loss : 2.402\n",
      "4608=> loss : 1.647\n",
      "4672=> loss : 1.921\n",
      "4736=> loss : 0.897\n",
      "4800=> loss : 1.363\n",
      "4864=> loss : 1.925\n",
      "4928=> loss : 1.435\n",
      "4992=> loss : 2.218\n",
      "5056=> loss : 1.389\n",
      "5120=> loss : 0.948\n",
      "5184=> loss : 2.033\n",
      "5248=> loss : 1.656\n",
      "5312=> loss : 1.544\n",
      "5376=> loss : 2.381\n",
      "5440=> loss : 1.860\n",
      "5504=> loss : 1.566\n",
      "5568=> loss : 1.427\n",
      "5632=> loss : 2.058\n",
      "5696=> loss : 1.750\n",
      "5760=> loss : 1.783\n",
      "5824=> loss : 0.973\n",
      "5888=> loss : 1.114\n",
      "5952=> loss : 2.265\n",
      "6016=> loss : 1.169\n",
      "6080=> loss : 1.458\n",
      "6144=> loss : 1.592\n",
      "6208=> loss : 1.595\n",
      "0=> loss : 1.604\n",
      "64=> loss : 1.058\n",
      "128=> loss : 1.453\n",
      "192=> loss : 1.193\n",
      "256=> loss : 1.689\n",
      "320=> loss : 0.694\n",
      "384=> loss : 1.802\n",
      "448=> loss : 1.824\n",
      "512=> loss : 1.218\n",
      "576=> loss : 0.719\n",
      "640=> loss : 1.236\n",
      "704=> loss : 1.945\n",
      "768=> loss : 1.480\n",
      "832=> loss : 0.984\n",
      "896=> loss : 1.111\n",
      "960=> loss : 0.774\n",
      "1024=> loss : 1.561\n",
      "1088=> loss : 1.684\n",
      "1152=> loss : 1.485\n",
      "1216=> loss : 1.191\n",
      "1280=> loss : 1.409\n",
      "1344=> loss : 1.708\n",
      "1408=> loss : 1.652\n",
      "1472=> loss : 0.916\n",
      "1536=> loss : 1.576\n",
      "1600=> loss : 1.162\n",
      "1664=> loss : 1.139\n",
      "1728=> loss : 1.366\n",
      "1792=> loss : 1.940\n",
      "1856=> loss : 1.492\n",
      "1920=> loss : 1.456\n",
      "1984=> loss : 1.567\n",
      "2048=> loss : 1.665\n",
      "2112=> loss : 1.760\n",
      "2176=> loss : 1.131\n",
      "2240=> loss : 1.265\n",
      "2304=> loss : 1.759\n",
      "2368=> loss : 1.987\n",
      "2432=> loss : 1.290\n",
      "2496=> loss : 1.492\n",
      "2560=> loss : 0.802\n",
      "2624=> loss : 1.450\n",
      "2688=> loss : 1.760\n",
      "2752=> loss : 1.248\n",
      "2816=> loss : 1.308\n",
      "2880=> loss : 0.802\n",
      "2944=> loss : 1.695\n",
      "3008=> loss : 1.130\n",
      "3072=> loss : 1.810\n",
      "3136=> loss : 0.911\n",
      "3200=> loss : 0.906\n",
      "3264=> loss : 1.081\n",
      "3328=> loss : 3.026\n",
      "3392=> loss : 0.863\n",
      "3456=> loss : 1.539\n",
      "3520=> loss : 1.427\n",
      "3584=> loss : 1.923\n",
      "3648=> loss : 0.470\n",
      "3712=> loss : 0.877\n",
      "3776=> loss : 1.216\n",
      "3840=> loss : 1.862\n",
      "3904=> loss : 0.686\n",
      "3968=> loss : 1.603\n",
      "4032=> loss : 1.340\n",
      "4096=> loss : 0.980\n",
      "4160=> loss : 0.838\n",
      "4224=> loss : 0.864\n",
      "4288=> loss : 1.858\n",
      "4352=> loss : 0.644\n",
      "4416=> loss : 1.274\n",
      "4480=> loss : 1.377\n",
      "4544=> loss : 0.911\n",
      "4608=> loss : 1.381\n",
      "4672=> loss : 0.843\n",
      "4736=> loss : 1.240\n",
      "4800=> loss : 1.860\n",
      "4864=> loss : 1.514\n",
      "4928=> loss : 1.336\n",
      "4992=> loss : 1.189\n",
      "5056=> loss : 1.286\n",
      "5120=> loss : 0.925\n",
      "5184=> loss : 1.077\n",
      "5248=> loss : 0.923\n",
      "5312=> loss : 1.392\n",
      "5376=> loss : 1.492\n",
      "5440=> loss : 0.902\n",
      "5504=> loss : 1.928\n",
      "5568=> loss : 2.597\n",
      "5632=> loss : 1.180\n",
      "5696=> loss : 1.533\n",
      "5760=> loss : 1.206\n",
      "5824=> loss : 1.720\n",
      "5888=> loss : 1.510\n",
      "5952=> loss : 1.201\n",
      "6016=> loss : 1.247\n",
      "6080=> loss : 1.079\n",
      "6144=> loss : 0.998\n",
      "6208=> loss : 1.091\n",
      "0=> loss : 1.380\n",
      "64=> loss : 1.234\n",
      "128=> loss : 1.189\n",
      "192=> loss : 1.493\n",
      "256=> loss : 1.119\n",
      "320=> loss : 1.235\n",
      "384=> loss : 1.189\n",
      "448=> loss : 1.350\n",
      "512=> loss : 1.507\n",
      "576=> loss : 1.108\n",
      "640=> loss : 1.639\n",
      "704=> loss : 0.874\n",
      "768=> loss : 1.130\n",
      "832=> loss : 1.327\n",
      "896=> loss : 0.939\n",
      "960=> loss : 1.962\n",
      "1024=> loss : 0.505\n",
      "1088=> loss : 1.202\n",
      "1152=> loss : 1.099\n",
      "1216=> loss : 0.831\n",
      "1280=> loss : 1.052\n",
      "1344=> loss : 1.483\n",
      "1408=> loss : 1.139\n",
      "1472=> loss : 1.437\n",
      "1536=> loss : 1.257\n",
      "1600=> loss : 1.778\n",
      "1664=> loss : 1.566\n",
      "1728=> loss : 1.380\n",
      "1792=> loss : 0.561\n",
      "1856=> loss : 1.258\n",
      "1920=> loss : 2.046\n",
      "1984=> loss : 0.998\n",
      "2048=> loss : 0.963\n",
      "2112=> loss : 0.484\n",
      "2176=> loss : 0.984\n",
      "2240=> loss : 2.151\n",
      "2304=> loss : 0.858\n",
      "2368=> loss : 0.820\n",
      "2432=> loss : 1.125\n",
      "2496=> loss : 0.433\n",
      "2560=> loss : 0.934\n",
      "2624=> loss : 1.419\n",
      "2688=> loss : 1.430\n",
      "2752=> loss : 1.787\n",
      "2816=> loss : 1.317\n",
      "2880=> loss : 1.276\n",
      "2944=> loss : 1.376\n",
      "3008=> loss : 1.183\n",
      "3072=> loss : 0.912\n",
      "3136=> loss : 0.845\n",
      "3200=> loss : 1.356\n",
      "3264=> loss : 1.935\n",
      "3328=> loss : 2.180\n",
      "3392=> loss : 1.133\n",
      "3456=> loss : 1.296\n",
      "3520=> loss : 1.188\n",
      "3584=> loss : 1.255\n",
      "3648=> loss : 0.736\n",
      "3712=> loss : 1.660\n",
      "3776=> loss : 1.518\n",
      "3840=> loss : 1.320\n",
      "3904=> loss : 0.909\n",
      "3968=> loss : 1.640\n",
      "4032=> loss : 0.866\n",
      "4096=> loss : 1.656\n",
      "4160=> loss : 0.907\n",
      "4224=> loss : 1.075\n",
      "4288=> loss : 1.213\n",
      "4352=> loss : 1.544\n",
      "4416=> loss : 2.169\n",
      "4480=> loss : 1.010\n",
      "4544=> loss : 1.535\n",
      "4608=> loss : 1.233\n",
      "4672=> loss : 1.039\n",
      "4736=> loss : 2.115\n",
      "4800=> loss : 1.409\n",
      "4864=> loss : 2.076\n",
      "4928=> loss : 1.002\n",
      "4992=> loss : 1.318\n",
      "5056=> loss : 1.748\n",
      "5120=> loss : 0.934\n",
      "5184=> loss : 1.268\n",
      "5248=> loss : 0.731\n",
      "5312=> loss : 0.981\n",
      "5376=> loss : 1.179\n",
      "5440=> loss : 1.544\n",
      "5504=> loss : 2.729\n",
      "5568=> loss : 0.598\n",
      "5632=> loss : 0.679\n",
      "5696=> loss : 0.876\n",
      "5760=> loss : 0.833\n",
      "5824=> loss : 0.460\n",
      "5888=> loss : 0.707\n",
      "5952=> loss : 1.826\n",
      "6016=> loss : 1.029\n",
      "6080=> loss : 1.305\n",
      "6144=> loss : 0.688\n",
      "6208=> loss : 0.663\n",
      "train over\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # GPU\n",
    "        # inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "        # 학습이 되기전에 0으로 초기화 해주는 부분\n",
    "        optim.zero_grad()\n",
    "        out = net(inputs)\n",
    "\n",
    "        loss = cost(out, labels)\n",
    "        \n",
    "        # 자동으로 각 parameters(w, b)들이 Gradient값을 가진다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 그 값을 가지고 parameters(w, b)들을 업데이트를 진행한다. \n",
    "        optim.step()\n",
    "\n",
    "        if i % 64 == 0:\n",
    "            print(\"%d=> loss : %.3f\"%(i, loss))\n",
    "\n",
    "print(\"train over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 평가 및 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: %f 63.89\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data \n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1) # torch.max( _, 1)은 각 행에서 가장 큰 수만 뽑아내는 것입니다.\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %f', (100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
